{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77abab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Studium\\HPI\\22-23 WS\\Social Media Mining\\Code\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907a8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_normalized(path):\n",
    "    with open(path) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    return pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e090bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85eed8",
   "metadata": {},
   "source": [
    "#### TODO: Define time splits here. Remove data not contained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb1b09",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07873014",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"src/Data_test/\"\n",
    "path_preprocessed = \"src/Data_test/preprocessed/\"\n",
    "# create folder if not existing\n",
    "os.makedirs(path_preprocessed, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe385a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = read_json_normalized(f\"{path}user.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c81d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user['created_at'] = pd.to_datetime(user['created_at'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a0a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_index={uid:index for index, uid in enumerate(user['id'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d34bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = pd.read_csv(f\"{path}edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79281182",
   "metadata": {},
   "outputs": [],
   "source": [
    "split=pd.read_csv(f\"{path}split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3f851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=pd.read_csv(f\"{path}labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a7ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_label={uid:label for uid, label in zip(label['id'].values,label['label'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f58a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_split={uid:split for uid, split in zip(split['id'].values,split['split'].values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524c5cf",
   "metadata": {},
   "source": [
    "### Create Labels und data masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd70c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 68869/68869 [00:00<00:00, 908594.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels: 50609\n",
      "Test Labels: 8967\n",
      "Validation Labels: 9293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels_new = []\n",
    "train_mask = []\n",
    "test_mask = []\n",
    "validation_mask = []\n",
    "\n",
    "for i, uid in enumerate(tqdm(uid_index.keys())):\n",
    "    user_label = uid_label[uid]\n",
    "    user_split = uid_split[uid]\n",
    "    \n",
    "    if user_label == \"human\":\n",
    "        labels_new.append(0)\n",
    "    else:\n",
    "        labels_new.append(1)\n",
    "        \n",
    "    if user_split == \"train\":\n",
    "        train_mask.append(i)\n",
    "    elif user_split == \"test\":\n",
    "        test_mask.append(i)\n",
    "    else:\n",
    "        validation_mask.append(i)\n",
    "        \n",
    "assert (len(train_mask) + len(test_mask) + len(validation_mask)) == len(uid_index)\n",
    "print(\"Train Labels: \" + str(len(train_mask)))\n",
    "print(\"Test Labels: \" + str(len(test_mask)))\n",
    "print(\"Validation Labels: \" + str(len(validation_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfce333",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(train_mask, dtype=torch.long), f\"{path_preprocessed}train_mask.pt\")\n",
    "torch.save(torch.tensor(test_mask, dtype=torch.long), f\"{path_preprocessed}test_mask.pt\")\n",
    "torch.save(torch.tensor(validation_mask, dtype=torch.long), f\"{path_preprocessed}validation_mask.pt\")\n",
    "torch.save(torch.tensor(labels_new, dtype=torch.long), f\"{path_preprocessed}labels.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a152a00",
   "metadata": {},
   "source": [
    "### Create Edge indices and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f3ce75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['followers', 'following'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge['relation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd588d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 138774/138774 [00:01<00:00, 91187.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Index: 138774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "edge_index = []\n",
    "edge_type = []\n",
    "#edge_relation_mapping = {'followers': 0, 'following': 1}\n",
    "edge_relation_mapping = {'retweet': 0, 'following': 1}\n",
    "\n",
    "for i in tqdm(range(len(edge))):\n",
    "    source_id = edge['source_id'][i]\n",
    "    target_id = edge['target_id'][i]\n",
    "    relation = edge['relation'][i]\n",
    "    if relation in edge_relation_mapping:\n",
    "        try:\n",
    "            edge_index.append([uid_index[source_id], uid_index[target_id]])\n",
    "            edge_type.append(edge_relation_mapping[relation])\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "assert len(edge_index) == len(edge_type)\n",
    "print(\"Edge Index: \" + str(len(edge_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38e46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(edge_index, dtype=torch.long).t(), f\"{path_preprocessed}edge_index_retweet.pt\")\n",
    "torch.save(torch.tensor(edge_type, dtype=torch.long), f\"{path_preprocessed}edge_type_retweet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcfbf62",
   "metadata": {},
   "source": [
    "### Create numerical and categorical feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0fb27c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at                                                2021-11-28 02:14:18+00:00\n",
      "description                                                           AI Researcher\n",
      "id                                                             u1464779562471112705\n",
      "location                                                                       None\n",
      "name                                                                 Thabang Lebese\n",
      "pinned_tweet_id                                                                 NaN\n",
      "profile_image_url                 https://pbs.twimg.com/profile_images/146477986...\n",
      "protected                                                                     False\n",
      "url                                                         https://t.co/oGmKkcIvGz\n",
      "username                                                                  TDLebese_\n",
      "verified                                                                      False\n",
      "withheld                                                                        NaN\n",
      "entities.url.urls                 [{'start': 0, 'end': 23, 'url': 'https://t.co/...\n",
      "public_metrics.followers_count                                                    1\n",
      "public_metrics.following_count                                                   19\n",
      "public_metrics.tweet_count                                                       15\n",
      "public_metrics.listed_count                                                       0\n",
      "entities                                                                        NaN\n",
      "entities.description.urls                                                       NaN\n",
      "entities.description.hashtags                                                   NaN\n",
      "entities.description.mentions                                                   NaN\n",
      "entities.description.cashtags                                                   NaN\n",
      "withheld.country_codes                                                          NaN\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(user.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa3c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numerical_feature(data):\n",
    "    data = np.array(data)\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return ((data - mean) / std).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e21e09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_user_property(property_name, normalize = False):\n",
    "    res = []\n",
    "    for e in user[property_name]:\n",
    "        if e is not None and e is not math.isnan(e):\n",
    "            res.append(e)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return normalize_numerical_feature(res) if normalize else res\n",
    "\n",
    "def extract_literal_user_property(property_name):\n",
    "    res = []\n",
    "    for e in user[property_name]:\n",
    "        if e is not None:\n",
    "            res.append(e)\n",
    "        else:\n",
    "            res.append(\"\")\n",
    "    return res\n",
    "\n",
    "def extract_boolean_user_property(property_name):\n",
    "    res = []\n",
    "    for e in user[property_name]:\n",
    "        if e == True:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "915d6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "following_count = extract_numeric_user_property('public_metrics.following_count', True)\n",
    "followers_count = extract_numeric_user_property('public_metrics.followers_count', True)\n",
    "tweet_count = extract_numeric_user_property('public_metrics.tweet_count', True)\n",
    "\n",
    "#username_length = list(map(lambda s: len(s), extract_literal_user_property('username'))) #not in use\n",
    "name_length = list(map(lambda s: len(s), extract_literal_user_property('name')))\n",
    "\n",
    "#normalize\n",
    "#username_length = normalize_numerical_feature(username_length)\n",
    "name_length = normalize_numerical_feature(name_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20afacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_date = dt.strptime('Tue Sep 5 00:00:00 +0000 2020 ','%a %b %d %X %z %Y ')\n",
    "start_date = dt.strptime('15/03/22 00:00:00 +0000','%d/%m/%y %H:%M:%S %z') #change to last date of dataset\n",
    "active_days = []\n",
    "for create_date in user['created_at']:\n",
    "    active_days.append((start_date - create_date).days)\n",
    "active_days = normalize_numerical_feature(active_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a66af62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "following_count = torch.tensor(following_count, dtype=torch.float32)\n",
    "followers_count = torch.tensor(followers_count, dtype=torch.float32)\n",
    "tweet_count = torch.tensor(tweet_count, dtype=torch.float32)\n",
    "# username_length = torch.tensor(username_length, dtype=torch.float32) #not in use\n",
    "name_length = torch.tensor(name_length, dtype=torch.float32)\n",
    "active_days = torch.tensor(active_days, dtype=torch.float32)\n",
    "\n",
    "num_properties_tensor = torch.cat([\n",
    "    followers_count,\n",
    "    active_days,\n",
    "    name_length,\n",
    "    following_count,\n",
    "    tweet_count], \n",
    "    dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f8967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaN values\n",
    "pd.DataFrame(num_properties_tensor.detach().numpy()).isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fad7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = extract_boolean_user_property('protected')\n",
    "verified = extract_boolean_user_property('verified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3554286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_profile_image = []\n",
    "default_image_url = 'https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png'\n",
    "for e in user['profile_image_url']:\n",
    "    if e is not None:\n",
    "        if e == default_image_url or e == '':\n",
    "            default_profile_image.append(1)\n",
    "        else:\n",
    "            default_profile_image.append(0)\n",
    "    else:\n",
    "        default_profile_image.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b704e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "protected = torch.tensor(protected, dtype=torch.float16).reshape(-1, 1)\n",
    "verified = torch.tensor(verified, dtype=torch.float16).reshape(-1, 1)\n",
    "default_profile_image = torch.tensor(default_profile_image, dtype=torch.float16).reshape(-1, 1)\n",
    "\n",
    "categorical_properties_tensor = torch.cat([\n",
    "    protected,\n",
    "    verified,\n",
    "    default_profile_image], \n",
    "    dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaaf5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(num_properties_tensor, f\"{path_preprocessed}num_properties_tensor.pt\")\n",
    "torch.save(categorical_properties_tensor, f\"{path_preprocessed}categorical_properties_tensor.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8a78f",
   "metadata": {},
   "source": [
    "### Extract user tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "176af418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weekly time split\n",
    "first_week = dt.strptime('20/09/21 00:00:00 +0000','%d/%m/%y %H:%M:%S %z') # Start: Week 38-2021\n",
    "number_of_weeks = 26 # End: Week 10-2022\n",
    "all_timestamps = []\n",
    "\n",
    "for i in range(number_of_weeks):\n",
    "    all_timestamps.append(first_week + timedelta(weeks=i))\n",
    "    \n",
    "id_tweet_timestamps = [{i:[] for i in range(len(uid_index.keys()))} for _ in range(len(all_timestamps))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "299ae424",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "user_tweets_path = \"src/Data_test/users/\"\n",
    "with open(f\"{user_tweets_path}u12/tweet.json\", 'r') as tweet_file:\n",
    "    tweets_ = json.load(tweet_file)\n",
    "    for tweet in tweets_:\n",
    "        timestamp = dt.strptime(tweet['created_at'], '%Y-%m-%d %H:%M:%S%z')\n",
    "        for idx in range(1, len(all_timestamps)):\n",
    "            before = all_timestamps[idx-1]\n",
    "            after = all_timestamps[idx]\n",
    "            if timestamp >= before and timestamp < after:\n",
    "                print(timestamp)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1866b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 68869/68869 [43:15<00:00, 26.53it/s]\n",
      "26it [00:29,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "number_of_users_with_tweets = 0\n",
    "\n",
    "counts = defaultdict(int)\n",
    "user_tweets_path = \"src/Data_test/users/\"\n",
    "for username in tqdm(list(uid_index.keys())):\n",
    "    tweet_path_specific = f\"{user_tweets_path}{username}/tweet.json\"\n",
    "    try:\n",
    "        u_id = uid_index[username]\n",
    "        with open(tweet_path_specific, 'r') as tweet_file:\n",
    "            tweets = json.load(tweet_file)\n",
    "            if len(tweets) > 0:\n",
    "                number_of_users_with_tweets += 1\n",
    "        for tweet in tweets:\n",
    "            text = tweet['text']\n",
    "            timestamp = dt.strptime(tweet['created_at'], '%Y-%m-%d %H:%M:%S%z')\n",
    "            for idx in range(1, len(all_timestamps)):\n",
    "                before = all_timestamps[idx-1]\n",
    "                after = all_timestamps[idx]\n",
    "                if timestamp >= before and timestamp < after:\n",
    "                    id_tweet_timestamps[idx-1][u_id].append(text)\n",
    "                    counts[idx-1] += 1\n",
    "                    break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "        \n",
    "for idx, t in tqdm(enumerate(id_tweet_timestamps)):\n",
    "    with open(f\"{path_preprocessed}id_tweet_{idx}.json\", 'w') as tweet_file:\n",
    "        json.dump(t, tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e90890e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63802"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_users_with_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf2da371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {20: 1003474,\n",
       "             18: 534902,\n",
       "             17: 399229,\n",
       "             16: 319977,\n",
       "             15: 240536,\n",
       "             10: 147259,\n",
       "             9: 114668,\n",
       "             22: 841797,\n",
       "             21: 651518,\n",
       "             8: 119609,\n",
       "             1: 72102,\n",
       "             0: 68483,\n",
       "             23: 1125186,\n",
       "             19: 817105,\n",
       "             12: 176174,\n",
       "             7: 114822,\n",
       "             6: 102911,\n",
       "             5: 97503,\n",
       "             4: 90940,\n",
       "             2: 82451,\n",
       "             14: 167701,\n",
       "             3: 85157,\n",
       "             13: 162539,\n",
       "             11: 163205,\n",
       "             24: 341283})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87d01536",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "user_tweets_path = \"src/Data_test/users/\"\n",
    "for username in tqdm(uid_index.keys()):\n",
    "    tweet_path_specific = f\"{user_tweets_path}{username}/tweet.json\"\n",
    "    try:\n",
    "        u_id = uid_index[username]\n",
    "        with open(tweet_path_specific, 'r') as tweet_file:\n",
    "            tweets = json.load(tweet_file)\n",
    "        for tweet in tweets:\n",
    "            text = tweet['text']\n",
    "            id_tweet[u_id].append(text)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "with open(f\"{path_preprocessed}id_tweet.json\", 'w') as tweet_file:\n",
    "    json.dump(id_tweet, tweet_file)\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32144e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_descriptions = list(user['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d71d9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses string as key\n",
    "#each_user_tweets=json.load(open(\"src/Data_test/preprocessed/id_tweet.json\",'r')) # = id_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eac61289",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tweet_timestamps = [json.load(open(f\"{path_preprocessed}id_tweet_{idx}.json\",'r')) for idx in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0384be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT @FrRonconi: This aircraft uses foldable wings to take off like a helicopter yet fly like a normal airplane 👍🏻\\n\\nby @pterodynamics \\n#Drone…', 'RT @enilev: Researchers Build #NeuralNetworks With Actual #Neurons \\nV/ @hackaday \\n\\n#Science #neurotech \\n@CorticalLabs \\n\\n@andi_staub @IanLJo…', 'RT @chboursin: This egg-shaped capsule is actually a smart, self-sustaining home v/ @mashable  cc @jblefevre60 @kalydeoo @FrRonconi @andi_s…', 'RT @sebbourguignon: [#Innovation] This robotic upper limb looks so realistic via @gigadgets_ \\n\\n#AI #Engineering\\n\\n@labordeolivier @MargaretS…', 'RT @jblefevre60: 💥Top 5 future technology inventions 2022-2050!\\n\\n#AI #MachineLearning #Python #coding #100DaysOfCode @MikeQuindazzi\\n\\n@Spiro…', 'RT @pascal_bornet: Breakthrough: thanks to this machine, this paralyzed man is now able to communicate using his thoughts\\n\\nA brilliant proj…', 'RT @VisiveAI: This is a #robot monster right out of #WildWildWest.\\n \\n#Robotics #Automation #Robot #ArtificialIntelligence #MachineLearning…', 'RT @labordeolivier: Future #Mobility Technology 🚀\\n#EV #AI #drone\\nby @takeonGravity &amp; @FrRonconi\\n\\n@sebbourguignon @HeinzVHoenen @Khulood_Alm…', 'RT @sebbourguignon: [#Innovation] This machine eliminates 100,000 weeds per hour using powerful lasers via @gigadgets_ \\n\\n#AI #Engineering…', 'RT @VisiveAI: This AI monitor beaches, marina &amp; even ports for safe movement of traffic\\n\\n#Robotics #Automation #Robot #ArtificialIntelligen…', 'RT @VisiveAI: This is a flamethrowing drone!  \\n\\n#Drones #Robotics #IoT #Engineering #AI By @digitaltrendsaz https://t.co/4s1KSRM8eY', 'RT @JeroenBartelse: California has issued permits for autonomous vehicle service to Cruise and Waymo\\n\\nhttps://t.co/dvr6UohBMq\\nhttps://t.co/…']\n"
     ]
    }
   ],
   "source": [
    "print(id_tweet_timestamps[24]['45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72b73174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "counts = defaultdict(int)\n",
    "number_of_users_with_tweets_timestamps = 0\n",
    "for i in range(26):\n",
    "    for j in range(len(id_tweet_timestamps[i])):\n",
    "        if len(id_tweet_timestamps[i][str(j)]) > 0:\n",
    "            counts[i] += 1\n",
    "            number_of_users_with_tweets_timestamps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a48b8156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 10873,\n",
       "             1: 11266,\n",
       "             2: 12266,\n",
       "             3: 12326,\n",
       "             4: 12781,\n",
       "             5: 13229,\n",
       "             6: 13971,\n",
       "             7: 14574,\n",
       "             8: 15004,\n",
       "             9: 14799,\n",
       "             10: 16761,\n",
       "             11: 17902,\n",
       "             12: 18490,\n",
       "             13: 17579,\n",
       "             14: 17034,\n",
       "             15: 21159,\n",
       "             16: 24638,\n",
       "             17: 27798,\n",
       "             18: 29015,\n",
       "             19: 32734,\n",
       "             20: 38767,\n",
       "             21: 42058,\n",
       "             22: 40329,\n",
       "             23: 37622,\n",
       "             24: 23332})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2f1591b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536307"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_users_with_tweets_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4d553",
   "metadata": {},
   "source": [
    "### Create feature embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af659ca7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_extraction_pipeline = pipeline('feature-extraction', model='roberta-base', tokenizer='roberta-base', device=device, padding=True, truncation=True, max_length=50, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0a21426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nempty_counter = 0\\nfor idx, (k, v) in tqdm(enumerate(each_user_tweets.items())):\\n    print(k)\\n    print(v)\\n    \\n    if idx == 10:\\n        break\\n    \\n    \\nprint(empty_counter)\\n#print(empty_counter / len(each_user_tweets))\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "empty_counter = 0\n",
    "for idx, (k, v) in tqdm(enumerate(each_user_tweets.items())):\n",
    "    print(k)\n",
    "    print(v)\n",
    "    \n",
    "    if idx == 10:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print(empty_counter)\n",
    "#print(empty_counter / len(each_user_tweets))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3790f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(each_user_tweets.keys())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2eea59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key = '3'\n",
    "#print(type(each_user_tweets[key]))\n",
    "#print(each_user_tweets[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "355fa81a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "user_description_embedding = []\n",
    "for desc in tqdm(user_descriptions):\n",
    "    if not desc or len(desc) == 0:\n",
    "        user_description_embedding.append(torch.zeros(768))\n",
    "        continue\n",
    "    feature = torch.tensor(text_extraction_pipeline(desc))\n",
    "    mean_feature = torch.mean(feature, dim=[0,1])\n",
    "    user_description_embedding.append(mean_feature)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "298d6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(torch.stack(user_description_embedding, dim=0), f\"{path_preprocessed}user_description_embedding_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "889aa61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]E:\\Studium\\HPI\\22-23 WS\\Social Media Mining\\Code\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "26it [16:00:03, 2215.52s/it]\n"
     ]
    }
   ],
   "source": [
    "max_tweets_per_user = 20\n",
    "from collections import defaultdict\n",
    "counts_enc = defaultdict(int)\n",
    "\n",
    "for idx, t_list in tqdm(enumerate(id_tweet_timestamps)):\n",
    "    tweets_list = []\n",
    "    for i in range(len(t_list)):\n",
    "        tweets = t_list[str(i)]\n",
    "        number_of_tweets = min(max_tweets_per_user, len(tweets))\n",
    "\n",
    "        if len(tweets) == 0:\n",
    "            mean_feature = torch.zeros(768)\n",
    "        else:\n",
    "            counts_enc[idx] += 1\n",
    "            tweet_embeddings = []\n",
    "            for j, tweet in enumerate(tweets[0:number_of_tweets]):\n",
    "\n",
    "                if not tweet or len(tweet) == 0:\n",
    "                    tweet_embeddings.append(torch.zeros(768))\n",
    "                    continue\n",
    "\n",
    "                each_tweet_tensor=torch.tensor(text_extraction_pipeline(tweet))\n",
    "                total_word_tensor = torch.mean(each_tweet_tensor, dim=[0,1])\n",
    "                tweet_embeddings.append(total_word_tensor)\n",
    "\n",
    "            mean_feature = torch.mean(torch.stack(tweet_embeddings), dim=0)\n",
    "        tweets_list.append(mean_feature)\n",
    "    torch.save(torch.stack(tweets_list), f\"{path_preprocessed}user_tweets_tensor_{idx}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcbc615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.mean(torch.stack(tweets_list, dim=0), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "745118dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(torch.stack(tweets_list), f\"{path_preprocessed}user_tweets_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f844e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1452a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_tweets_per_user = 20\n",
    "tweets_list = []\n",
    "for i in tqdm(range(len(each_user_tweets))):\n",
    "    tweets = each_user_tweets[str(i)]\n",
    "    number_of_tweets = min(max_tweets_per_user, len(tweets))\n",
    "    \n",
    "    if len(tweets) == 0:\n",
    "        mean_feature = torch.zeros(768)\n",
    "    else:\n",
    "        tweet_embeddings = []\n",
    "        for j, tweet in enumerate(tweets[0:number_of_tweets]):\n",
    "            if not tweet or len(tweet) == 0:\n",
    "                tweet_embeddings.append(torch.zeros(768))\n",
    "                continue\n",
    "\n",
    "            each_tweet_tensor=torch.tensor(text_extraction_pipeline(tweet))\n",
    "            total_word_tensor = torch.mean(each_tweet_tensor, dim=[0,1])\n",
    "            tweet_embeddings.append(total_word_tensor)\n",
    "            \n",
    "        mean_feature = torch.mean(torch.stack(tweet_embeddings), dim=0)\n",
    "    tweets_list.append(mean_feature)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55656f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
